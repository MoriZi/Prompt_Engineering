{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outline**\n",
    "\n",
    "- Prompting Techniques: Zero-shot, Few-shot, Chain-of-Thought\n",
    "- Hands-On Exercises: Crafting Complex Prompts\n",
    "- Introduction to Prompt Optimization Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prompting Techniques**\n",
    "\n",
    "- Prompt Engineering optimizes prompt design for enhanced LLM task performance.\n",
    "- Transition from basic to advanced techniques for complex task achievement.\n",
    "- Focus on improving LLM reliability and performance through advanced prompting.\n",
    "\n",
    "- There are different techniques of prompting\n",
    "- the most popular and common ones are:\n",
    "  - Zero-shot \n",
    "  - Few-shot \n",
    "  - Chain-of-Though\n",
    "  - Self-Consistency\n",
    "  - Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Zero-Shot Prompting**\n",
    "\n",
    "- Large LLMs like GPT-3 excel in \"zero-shot\" tasks due to extensive training and data.\n",
    "- Capable of understanding and executing instructions without prior examples.\n",
    "- Illustration provided with a zero-shot example from the previous section.\n",
    "\n",
    "```\n",
    "Classify the text into neutral, negative or positive. \n",
    "\n",
    "Text: I think the vacation is okay.\n",
    "Sentiment:\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "Neutral\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zero-shot capability allows LLMs to understand tasks like \"sentiment\" analysis without prior examples.\n",
    "- Instruction tuning can enhance zero-shot learning, as demonstrated by Wei et al. (2022).\n",
    "- This involves finetuning models with datasets described through instructions.\n",
    "- RLHF (reinforcement learning from human feedback) scales instruction tuning by aligning models with human preferences, exemplified by ChatGPT.\n",
    "  - Discussion on instruction tuning and RLHF methods to follow in upcoming sections.\n",
    "- For tasks where zero-shot is insufficient, \n",
    "  - Few-shot prompting with examples improves performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Category Classification**\n",
    "**Objective:** Create a prompt for the model to classify a list of items into predefined categories without any examples.\n",
    "- **Task:** Given a list of items (e.g., \"apple,\" \"Google,\" \"Amazon River\"), students must design a prompt that asks the model to classify each item into categories such as 'fruit', 'company', or 'geographical feature'.\n",
    "- **Focus:** Understanding how to implicitly guide the model towards using common knowledge for classification.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "### **Exercise 2: Sentiment Analysis**\n",
    "**Objective:** Develop a prompt to determine the sentiment of a given piece of text.\n",
    "- **Task:** Students will write a prompt asking the model to analyze the sentiment of a review or opinion piece and conclude whether the sentiment is positive, negative, or neutral.\n",
    "- **Focus:** Crafting questions that lead the model to provide sentiment analysis based on the text's tone and content without relying on training examples.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### **Exercise 3: Summarization**\n",
    "**Objective:** Formulate a prompt that instructs the model to summarize a given article or paragraph.\n",
    "- **Task:** Provide a lengthy article or paragraph. Students must create a prompt asking the model to summarize the key points succinctly.\n",
    "- **Focus:** Encouraging conciseness and the ability to identify main ideas without prior examples of summarization.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### **Exercise 4: Question Answering**\n",
    "**Objective:** Design a prompt for the model to answer questions based on a provided text.\n",
    "- **Task:** Offer a short article or passage. Students need to generate a prompt that asks the model to answer questions about the text without having seen answers to similar questions before.\n",
    "- **Focus:** Navigating the model to understand context and extract factual information accurately.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### **Exercise 5: Creative Writing**\n",
    "**Objective:** Craft a prompt that encourages the model to generate a short story or poem based on a given theme or opening line.\n",
    "- **Task:** Students will formulate a prompt that guides the model to create a creative piece (e.g., a short story about a lost key or a poem about the sea).\n",
    "- **Focus:** Exploring the model's ability to engage in creative tasks and generate novel content based on minimal input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Few-Shot Prompting**\n",
    "\n",
    "- Large language models excel in zero-shot tasks but may struggle with complex challenges.\n",
    "- Few-shot prompting enhances performance by providing **in-context** learning through examples.\n",
    "- Demonstrations in prompts serve as conditioning for better model responses.\n",
    "- Few-shot capabilities emerged as models scaled in size, per Kaplan et al., 2020.\n",
    "- Example from Brown et al., 2020, illustrates few-shot prompting to use a new word correctly in a sentence.\n",
    "```\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    " \n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "\n",
    "```\n",
    "\n",
    "- The model can learn tasks from a single example, known as 1-shot learning.\n",
    "- For complex tasks, effectiveness may improve with more examples (3-shot, 5-shot, 10-shot, etc.).\n",
    "- Experimentation with the number of demonstrations can optimize task performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demonstrations' label space and input text distribution significantly impact model performance, as found by Min et al. (2022).\n",
    "- Correctness of labels for individual inputs is less critical than the overall structure provided by demonstrations.\n",
    "- The format of demonstrations, including the use of labels, enhances performance—even random labels are beneficial.\n",
    "- Opting for a true distribution of labels over a uniform distribution further improves task execution.\n",
    "\n",
    "- Experimenting with random label assignments to inputs.\n",
    "- Utilizes labels \"Negative\" and \"Positive\" without regard to their actual relevance.\n",
    "- This approach tests the impact of label format on model understanding and response quality.\n",
    "\n",
    "- **Example**\n",
    "```\n",
    "This is awesome! // Negative\n",
    "This is bad! // Positive\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! //\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correct answers achieved despite randomized label assignment.\n",
    "- Maintaining a consistent format contributes to successful outcomes.\n",
    "- Newer GPT models exhibit increased robustness to variations in formatting.\n",
    "- Further experimentation reveals improved model adaptability to random formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Positive This is awesome! \n",
    "This is bad! Negative\n",
    "Wow that movie was rad!\n",
    "Positive\n",
    "What a horrible show! --\n",
    "```\n",
    "\n",
    "- The model correctly predicts labels despite inconsistent formatting.\n",
    "- Indicates newer GPT models' ability to interpret and adapt to varied prompt structures.\n",
    "- A more detailed analysis is required to assess performance across diverse and complex tasks.\n",
    "- Investigating different prompt variations will help understand model robustness and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Data Classification**\n",
    "**Objective:** Teach the model to classify news articles into categories.\n",
    "- **Task:** Provide the model with a few examples of news headlines followed by their corresponding categories (e.g., \"Politics,\" \"Technology,\" \"Sports\"). Then, give it a new headline and ask it to classify it without explicitly stating the category options.\n",
    "- **Focus:** Understanding how the model uses examples to categorize new instances.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### **Exercise 2: Sentiment Analysis of Product Reviews**\n",
    "**Objective:** Perform sentiment analysis on product reviews.\n",
    "- **Task:** Give the model a few examples of product reviews with a labeled sentiment (positive, negative, neutral). Next, present an unlabeled review and ask the model to determine its sentiment.\n",
    "- **Focus:** Exploring how providing examples influences the model's ability to accurately gauge sentiment.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### **Exercise 3: Text Summarization**\n",
    "**Objective:** Summarize text passages.\n",
    "- **Task:** Start with a few examples of text passages alongside their summaries. Then, provide a new passage without a summary and ask the model to summarize it.\n",
    "- **Focus:** Learning how the model can condense information while retaining key points, influenced by the example summaries provided.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### **Exercise 4: Question Answering from Text**\n",
    "**Objective:** Extract answers from a provided text.\n",
    "- **Task:** Supply the model with a few examples of text passages followed by a question and its answer based on the passage. After these examples, give a new passage and question, asking the model to find the answer.\n",
    "- **Focus:** Demonstrating how the model applies learned patterns to extract information from new texts.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### Exercise 5: **Creative Writing Prompts**\n",
    "**Objective:** Generate creative texts based on prompts.\n",
    "- **Task:** Provide a few examples of prompts followed by creative responses (e.g., a prompt about a magical forest and a short story response). Then, offer a new prompt and ask the model to create a response.\n",
    "- **Focus:** Showcasing how examples influence the style, tone, and content of creative outputs.\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Limitations of Few-shot Prompting**\n",
    "- Few-shot prompting is effective for various tasks but has limitations with complex reasoning.\n",
    "- Demonstrating these limitations through a previously discussed example task.\n",
    "- The example will illustrate the challenges few-shot prompting faces with intricate reasoning scenarios.\n",
    "\n",
    "```\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "\n",
    "A: \n",
    "```\n",
    "Output:\n",
    "```\n",
    "Yes, the odd numbers in this group add up to 107, which is an even number.\n",
    "```\n",
    "\n",
    "- Incorrect responses underscore the limitations of current language models.\n",
    "- Indicates a pressing need for advanced prompt engineering techniques.\n",
    "- Enhancing prompt design can address shortcomings in complex task handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Limitations of Few-shot Prompting**\n",
    "\n",
    "- Another example:\n",
    "```\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "The answer is True.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Limitations of Few-shot Prompting**\n",
    "\n",
    "- Few-shot prompting shows limitations in handling complex reasoning problems.\n",
    "- The task's complexity requires breaking down the problem into manageable steps.\n",
    "- Chain-of-Thought (CoT) prompting emerges as a solution for intricate tasks involving arithmetic, commonsense, and symbolic reasoning.\n",
    "- Demonstrating the problem-solving process step-by-step helps the model generate more accurate and reliable responses.\n",
    "\n",
    "- Examples in prompts aid in solving certain tasks effectively.\n",
    "- Limitations of zero-shot and few-shot prompting suggest a need for further model learning or adaptation.\n",
    "- Fine-tuning models or exploring advanced prompting techniques becomes necessary for improvement.\n",
    "- Introduction to Chain-of-Thought (CoT) prompting as a next step, highlighting its growing popularity in tackling complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chain-of-Thought (CoT) Prompting**\n",
    "\n",
    "- Chain-of-Thought (CoT) prompting enhances complex reasoning by outlining intermediate steps.\n",
    "- Combining CoT with few-shot prompting improves outcomes on tasks needing detailed reasoning.\n",
    "- This approach aids models in navigating through complex problems before generating responses.\n",
    "\n",
    "\n",
    "<img src=\"./images/cot.webp\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "Image Source: Wei et al. (2022)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Providing reasoning steps significantly improves model accuracy.\n",
    "- Demonstrates the effectiveness of CoT prompting with minimal examples.\n",
    "- A single example can suffice to guide the model to correct conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\n",
    "```\n",
    "\n",
    "- The emergence of advanced reasoning capabilities is attributed to the scale of language models.\n",
    "- Authors note that sufficiently large models develop the ability to follow complex reasoning paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Problem:\n",
    "\n",
    "Consider the question: **\"A farmer has 45 apples. If he sells them in bundles of 9, how many bundles can he sell, and how many apples will he have left?\"**\n",
    "\n",
    "This problem involves division and modular arithmetic, providing a clear example for CoT prompting across different exercises.\n",
    "\n",
    "### **Exercise 1: Arithmetic Word Problems**\n",
    "\n",
    "- **CoT Example Prompt:**\n",
    "  \"To solve this, first determine how many bundles of 9 apples can be made from 45 apples. Divide 45 by 9 to get the number of bundles. Then, calculate the remainder to find out how many apples are left.\"\n",
    "\n",
    "- **Expected Step-by-Step Response:** \"45 divided by 9 equals 5, so the farmer can sell 5 bundles. There's no remainder, so he has 0 apples left.\"\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### **Exercise 2: Multi-Step Logical Reasoning**\n",
    "\n",
    "- **CoT Example Prompt:**\n",
    "  \"Let's think through this logically. The farmer divides his apples into groups of 9. How many groups does he get? After distributing apples into these groups, are there any apples that can't be grouped?\"\n",
    "\n",
    "- **Expected Step-by-Step Response:**\n",
    "  \"By dividing 45 by 9, the farmer forms 5 complete groups. Since 45 is a multiple of 9, there are no apples left ungrouped.\"\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### **Exercise 3: Scientific Reasoning**\n",
    "\n",
    "- **CoT Example Prompt:**\n",
    "  \"To understand how many bundles the farmer can sell, let's apply a concept of division, which is similar to distributing a set of items evenly. After dividing the apples into bundles of 9, we'll use the concept of remainder to see if any apples are left.\"\n",
    "\n",
    "- **Expected Step-by-Step Response:**\n",
    "  \"Distributing 45 apples into bundles of 9 evenly, we get 5 bundles because 45 divided by 9 equals 5. The concept of remainder shows us there are 0 apples left, as 45 is evenly divisible by 9.\"\n",
    "\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### **Exercise 4: Historical Event Analysis**\n",
    "\n",
    "- **CoT Example Prompt:**\n",
    "  \"Imagine if in history, distributing resources efficiently was crucial for survival. Using this problem, calculate how the farmer can distribute his apples in bundles of 9, ensuring none are wasted.\"\n",
    "\n",
    "- **Expected Step-by-Step Response:**\n",
    "  \"In historical terms, the farmer would divide his 45 apples by 9, resulting in 5 bundles. This efficient distribution means no apples are wasted, aligning with historical needs for resource management.\"\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "### **Exercise 5: Ethical Dilemma Resolution**\n",
    "\n",
    "- **CoT Example Prompt:**\n",
    "  \"Consider the farmer's decision ethically: he must distribute apples fairly without waste. How can he achieve this by selling them in bundles of 9?\"\n",
    "\n",
    "- **Expected Step-by-Step Response:**\n",
    "  \"Ethically, the farmer seeks to avoid waste. By dividing 45 apples into bundles of 9, he creates 5 bundles. This division results in no waste, which is the ethically optimal outcome as all apples are used.\"\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Zero-shot COT Prompting**\n",
    "\n",
    "- Zero-shot Chain-of-Thought (CoT) introduces \"Let's think step by step\" to prompts.\n",
    "- Concept by Kojima et al. 2022 aims to enable reasoning without prior examples.\n",
    "- Testing this approach with a simple problem evaluates the model's reasoning performance.\n",
    "\n",
    "<img src=\"./images/zero-cot.webp\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "Image Source: Kojima et al. (2022)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "```\n",
    " \n",
    "- The answer is incorrect! Now Let's try with the special prompt.\n",
    "\n",
    "``` \n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "\n",
    "```\n",
    "\n",
    "- The simplicity and effectiveness of the zero-shot CoT prompt are notable.\n",
    "- Valuable for situations with limited examples available for prompting.\n",
    "- Demonstrates the potential for efficient reasoning in minimal-setup scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Problem for Exercise:**\n",
    "\"A farmer has 45 apples. If he sells them in bundles of 9, how many bundles can he sell, and how many apples will he have left?\"\n",
    "\n",
    "\n",
    "**Zero-shot CoT Prompt:**\n",
    "\"Think step-by-step to solve this problem: A farmer has 45 apples and wants to sell them in bundles of 9. How can we calculate the number of bundles he can sell and if any apples will be left? Explain your reasoning process clearly.\"\n",
    "\n",
    "**Expected CoT Response Example:**\n",
    "\n",
    "1. **Identify the Problem:** \"First, understand that the farmer is trying to divide 45 apples into as many bundles of 9 as possible.\"\n",
    "   \n",
    "2. **Determine the Operation Needed:** \"This requires division, where 45 is divided by 9 to find out how many complete bundles can be made.\"\n",
    "   \n",
    "3. **Perform the Division:** \"45 divided by 9 equals 5. This means the farmer can make 5 complete bundles of apples.\"\n",
    "   \n",
    "4. **Check for Remainders:** \"Next, we need to see if there are any apples left after making these bundles. Since 45 is evenly divisible by 9, there are 0 apples left.\"\n",
    "   \n",
    "5. **Conclude the Solution:** \"So, the farmer can sell 5 bundles of apples and will have no apples left over.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Self-Consistency**\n",
    "  - Proposed by Wang et al. (2022) to enhance Chain-of-Thought (CoT) prompting.\n",
    "  - Moves beyond simplistic greedy decoding by incorporating multiple, diverse reasoning paths.\n",
    "  - Technique involves sampling various reasoning paths via few-shot CoT.\n",
    "  - The most consistent answer among these generations is selected.\n",
    "  - Significantly improves performance on arithmetic and commonsense reasoning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output 1: \n",
    "\n",
    "```\n",
    "\n",
    "When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70 - 3 = 67. The answer is 67.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise:** Evaluating Self-Consistency in Weather Forecast Interpretations\n",
    "\n",
    "**Objective:** Assess the AI model's self-consistency by asking it to interpret weather forecasts in multiple ways.\n",
    "\n",
    "**Base Scenario for Exercise:**\n",
    "\"You're planning a picnic and decide to check the weather forecast for the weekend. The forecast indicates a 60% chance of rain on Saturday and a 20% chance of rain on Sunday.\"\n",
    "\n",
    "****Task 1:** Direct Interpretation**\n",
    "- **Prompt:** \"Based on the weather forecast, which day is better for a picnic, Saturday or Sunday? Explain your reasoning.\"\n",
    "- **Expected Response Traits:** The model should analyze the percentages and suggest that Sunday is better due to the lower chance of rain, explaining its reasoning based on the given probabilities.\n",
    "\n",
    "****Task 2:** Inverted Questioning**\n",
    "- **Prompt:** \"Given a 60% chance of rain on Saturday and a 20% chance on Sunday, on which day is a picnic more likely to be disrupted by rain?\"\n",
    "- **Expected Response Traits:** Despite the inversion of the question, the model should remain consistent with its earlier reasoning, indicating that Saturday is more likely to see disruption due to the higher chance of rain.\n",
    "\n",
    "****Task 3:** Scenario-Based Query**\n",
    "- **Prompt:** \"If I want to avoid rain during my picnic this weekend, what day should I choose based on the forecast?\"\n",
    "- **Expected Response Traits:** The model should recommend Sunday, aligning with its previous answers by prioritizing the day with the lower risk of rain.\n",
    "\n",
    "****Task 4:** Probability Assessment**\n",
    "- **Prompt:** \"What is the likelihood of having a dry picnic if held on Sunday, considering the weather forecast?\"\n",
    "- **Expected Response Traits:** The model should calculate or infer an 80% chance of a dry day, maintaining consistency in its assessment of Sunday as the preferable day for a picnic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generated Knowledge Prompting**\n",
    "\n",
    "\n",
    "<img src=\"./images/gen-knowledge.webp\" width=\"800\" align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generating Knowledge for Enhanced Predictions**\n",
    "  - LLMs have been enhanced with capabilities to incorporate external knowledge or information, enhancing the accuracy of their predictions.\n",
    "  - Liu et al. (2022) explored the use of LLMs to generate knowledge that is then integrated into the prompt, particularly to aid in tasks requiring commonsense reasoning.\n",
    "  - This approach is aimed at bolstering the model's ability to handle commonsense reasoning tasks by providing it with generated knowledge, thus enriching the context for more informed decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/img1.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- **Addressing LLM Limitations with Knowledge Generation**\n",
    "  - Recognizing LLMs' challenges in tasks requiring extensive world knowledge.\n",
    "  - Improvement strategy involves generating additional knowledge inputs.\n",
    "  - Initial step: Creation of several \"knowledge\" pieces to inform the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First, we generate a few \"knowledges\":**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/img2.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "<img src=\"./images/img3.png\" width=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The next step is to integrate the knowledge and get a prediction.**\n",
    "\n",
    "<img src=\"./images/img4.png\" width=\"800\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example in Document Analysis**\n",
    "\n",
    "- **Scenario Application**: Answering questions based on extensive text documents.\n",
    "- **Operational Steps**:\n",
    "- **Step 1: Quote Extraction**: Design the first prompt to identify and extract pertinent quotes from the document that are relevant to the posed question.\n",
    "- **Step 2: Question Answering**: The second prompt uses the extracted quotes alongside the original document as inputs to comprehensively answer the question.\n",
    "- **Benefits**:\n",
    "- **Focused Analysis**: By isolating relevant information through quote extraction, the LLM can concentrate on analyzing specific content that directly relates to the question.\n",
    "- **Enhanced Accuracy**: Separating the task into two distinct operations allows for more precise and contextually aware responses.\n",
    "- **Outcome**: This two-step approach ensures a thorough examination of the document, leveraging the strengths of the LLM in parsing and understanding complex texts to deliver accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv openai-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on Mac\n",
    "!source openai-env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (1.13.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (2.6.3)\n",
      "Requirement already satisfied: sniffio in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API key can be imported by running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI()\n",
    "# defaults to getting the key using os.environ.get(\"OPENAI_API_KEY\")\n",
    "# if you saved the key under a different environment variable name, you can do something like:\n",
    "client = OpenAI(\n",
    "  api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Step: Sending a Request to OpenAI API:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='In the realm of code, a concept profound,\\nRecursion whispers without a sound.\\nA function that calls itself with grace,\\nIn an infinite loop, it loves to chase.\\n\\nLike a mirror reflecting its own reflection,\\nRecursion dives deep without objection.\\nA task divided into smaller parts,\\nRepeating a cycle that never departs.\\n\\nEach recursive call a journey anew,\\nUnraveling problems, like a thread that grew.\\nThrough stacks and frames, it travels far,\\nUnfolding mysteries, like a shining star.\\n\\nA dance of logic, elegant and neat,\\nA loop of self-discovery, oh so sweet.\\nIn the world of programming, recursion stands tall,\\nA poetic symphony, enchanting all.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_open_params(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=256,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "):\n",
    "    \"\"\" set openai parameters\"\"\"\n",
    "\n",
    "    openai_params = {}    \n",
    "\n",
    "    openai_params['model'] = model\n",
    "    openai_params['temperature'] = temperature\n",
    "    openai_params['max_tokens'] = max_tokens\n",
    "    openai_params['top_p'] = top_p\n",
    "    openai_params['frequency_penalty'] = frequency_penalty\n",
    "    openai_params['presence_penalty'] = presence_penalty\n",
    "    return openai_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(gpt_client, params, messages):\n",
    "    \"\"\" GET completion from openai api\"\"\"\n",
    "\n",
    "    response = gpt_client.chat.completions.create(\n",
    "        model = params['model'],\n",
    "        messages = messages,\n",
    "        temperature = params['temperature'],\n",
    "        max_tokens = params['max_tokens'],\n",
    "        top_p = params['top_p'],\n",
    "        frequency_penalty = params['frequency_penalty'],\n",
    "        presence_penalty = params['presence_penalty'],\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Basic prompt example:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Antibiotics are medications that treat bacterial infections by either killing the bacteria or preventing them from reproducing, but they are not effective against viral infections and should be used appropriately to avoid antibiotic resistance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = set_open_params(temperature=0.7)\n",
    "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance. \n",
    "\n",
    "Explain the above in one sentence:\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(client, params, messages)\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Instruct the model to explain the paragraph in one sentence like \"I am 5\". Do you see any differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Few-shot prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The answer is True."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(client, params, messages)\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chain-of-Thought (CoT) Prompting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(client, params, messages)\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Zero-shot CoT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Initially, you had 10 apples. \n",
       "You gave 2 to the neighbor and 2 to the repairman, so you were left with 10 - 2 - 2 = 6 apples.\n",
       "After buying 5 more apples, you had 6 + 5 = 11 apples.\n",
       "Finally, after eating 1 apple, you were left with 11 - 1 = 10 apples. \n",
       "\n",
       "Therefore, you remained with 10 apples."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(client, params, messages)\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Self-Consistency**\n",
    "- As an exercise, check examples we had before and try them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When you were 6, your sister was half your age, so she was 3 years old. The age difference between you and your sister is 3 years. Now that you are 70, your sister must be 67 years old."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A:\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(client, params, messages)\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate Knowledge Prompting**\n",
    "- As an exercise, check examples we had before and try them here.\n",
    "\n",
    "\n",
    "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
    "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
    "\n",
    "Input: A rock is the same size as a pebble.\n",
    "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
    "\n",
    "Input: Part of golf is trying to get a higher point total than others.\n",
    "Knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Incorrect. In golf, the objective is to complete each hole in the fewest number of strokes possible, not to achieve a higher point total than others. The player with the lowest total score at the end of the round or tournament is the winner."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt= \"\"\"\n",
    "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
    "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
    "\n",
    "Input: A rock is the same size as a pebble.\n",
    "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
    "\n",
    "Input: Part of golf is trying to get a higher point total than others.\n",
    "Knowledge:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response_1 = get_completion(client, params, messages)\n",
    "Markdown(response_1.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Actually, in golf, the goal is to have the lowest score possible. Players aim to complete each hole in as few strokes as possible, with the overall objective being to have the lowest total score at the end of the round."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2 = get_completion(client, params, messages)\n",
    "Markdown(response_2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "No, the objective in golf is not to have a higher point total than others. The goal is actually to have the lowest score possible by completing each hole in as few strokes as possible. The player with the lowest total score at the end of the round or tournament is the winner."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt= \"\"\"\n",
    "Part of golf is trying to get a higher point total than others.Yes or No?\n",
    "Knowledge: In golf, the objective is to complete each hole in the fewest number of strokes possible, not to achieve a higher point total than others. The player with the lowest total score at the end of the round or tournament is the winner.Actually, in golf, the goal is to have the lowest score possible. Players aim to complete each hole in as few strokes as possible, with the overall objective being to have the lowest total score at the end of the round.\n",
    "Explain and Answer\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response_1 = get_completion(client, params, messages)\n",
    "Markdown(response_1.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Retrieval-Augmented Generation (RAG):**\n",
    "  - RAG represents a major advancement in conversational AI technology.\n",
    "  - It combines the capabilities of retrieval-based and generative models.\n",
    "\n",
    "- **Enhanced Interaction:**\n",
    "  - RAG improves machine-human interaction by offering more adaptable responses.\n",
    "  - Enables conversational AI to provide precise, contextually relevant answers.\n",
    "\n",
    "- **Leveraging External Information:**\n",
    "  - Draws on a wide range of external sources for information.\n",
    "  - Complements the model's internal knowledge for comprehensive responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **RAG Operation Workflow:**\n",
    "  - Begins with the formulation of the user's query.\n",
    "  - Searches large databases to find relevant information.\n",
    "  - Selects the most pertinent data for the given query.\n",
    "\n",
    "- **Generating Informed Responses:**\n",
    "  - Creates a response informed by the selected external knowledge.\n",
    "  - Ensures responses are not only plausible but also factually accurate.\n",
    "\n",
    "- **Application in Question-Answering Systems:**\n",
    "  - Particularly beneficial for systems requiring current and reliable facts.\n",
    "  - Enhances the accuracy and reliability of answers provided to users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LLMs as Knowledge Repositories:**\n",
    "  - Serve as encyclopedic sources with broad general knowledge.\n",
    "  - Often lack depth in specific, localized contexts.\n",
    "\n",
    "- **Limitations of LLMs in Specialized Contexts:**\n",
    "  - Inability to access specifics of internal databases or specialized research.\n",
    "  - Example given: OpenAI Innovation's detailed personnel information or proprietary data.\n",
    "\n",
    "- **RAG:**\n",
    "  - Bridges the gap between LLM's broad knowledge and specific, detailed information.\n",
    "  - Connects LLMs with context-rich data from discrete sources, such as text files or specific document compendiums.\n",
    "\n",
    "- **Application in Conversational AI:**\n",
    "  - Facilitates the creation of chatbots capable of responding with high accuracy and specificity to queries.\n",
    "  - Enhances chatbot responses by integrating detailed data from specialized sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Utilizing a Text Repository:**\n",
    "  - A comprehensive text repository contains all relevant information about OpenAI Innovation.\n",
    "\n",
    "- **RAG System Application:**\n",
    "  - Enables a chatbot to assist users by navigating through the repository.\n",
    "  - Provides informed responses to inquiries related to the company.\n",
    "\n",
    "- **Capabilities of the RAG-Equipped Chatbot:**\n",
    "  - Answers questions about the organizational structure, project specifics, and company policies.\n",
    "  - Draws precise information directly from OpenAI Innovation’s curated knowledge base.\n",
    "\n",
    "- **Benefit to Users:**\n",
    "  - Offers users accurate and specific information about OpenAI Innovation, enhancing user experience and support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Setting Up Your RAG System: Installing Dependencies**\n",
    "\n",
    "- **Essential Python Libraries:**\n",
    "  - **langchain:** Toolkit for working with language models.\n",
    "  - **openai:** Official OpenAI Python client for API interaction.\n",
    "  - **tiktoken:** Provides an efficient BPE tokenizer compatible with OpenAI model architectures.\n",
    "  - **faiss-gpu:** Library for fast similarity searching and clustering of dense vectors, optimized for GPUs.\n",
    "  - **langchain_experimental:** Contains experimental features for langchain.\n",
    "  - **langchain[docarray]:** Adds support for managing complex document structures within langchain.\n",
    "\n",
    "- **Purpose of Libraries:**\n",
    "  - Enable access to the OpenAI API.\n",
    "  - Facilitate data handling and operation of retrieval models.\n",
    "  - Enhance text encoding/decoding for NLP tasks.\n",
    "  - Improve efficiency in similarity searching and clustering with GPU support.\n",
    "\n",
    "- **Installation Note:**\n",
    "  - These libraries collectively prepare the Python environment for querying the RAG system, ensuring compatibility and performance optimization for NLP and retrieval tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.11-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.28-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.25 (from langchain)\n",
      "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.29 (from langchain)\n",
      "  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.22-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.8/114.8 kB\u001b[0m \u001b[31m686.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain) (2.6.3)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (23.2)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.9.15-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.1.11-py3-none-any.whl (807 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp311-cp311-macosx_11_0_arm64.whl (387 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.7/387.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.22-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading SQLAlchemy-2.0.28-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading orjson-3.9.15-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (248 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: urllib3, tenacity, SQLAlchemy, PyYAML, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, frozenlist, charset-normalizer, attrs, yarl, typing-inspect, requests, jsonpatch, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.28 aiohttp-3.9.3 aiosignal-1.3.1 attrs-23.2.0 charset-normalizer-3.3.2 dataclasses-json-0.6.4 frozenlist-1.4.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.11 langchain-community-0.0.27 langchain-core-0.1.30 langchain-text-splitters-0.0.1 langsmith-0.1.22 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.9.15 requests-2.31.0 tenacity-8.2.3 typing-inspect-0.9.0 urllib3-2.2.1 yarl-1.9.4\n",
      "Requirement already satisfied: openai in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (1.13.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (2.6.3)\n",
      "Requirement already satisfied: sniffio in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Using cached regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Downloading tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl (949 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.8/949.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.12.25 tiktoken-0.6.0\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Downloading faiss_cpu-1.8.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n",
      "Collecting langchain_experimental\n",
      "  Downloading langchain_experimental-0.0.53-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.8 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain_experimental) (0.1.11)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.27 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain_experimental) (0.1.30)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.0.27)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.1.22)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_experimental) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_experimental) (23.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_experimental) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_experimental) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.8->langchain_experimental) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.8->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.8->langchain_experimental) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.8->langchain_experimental) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.8->langchain_experimental) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.8->langchain_experimental) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.8->langchain_experimental) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.8->langchain_experimental) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.8->langchain_experimental) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.8->langchain_experimental) (2024.2.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.0.0)\n",
      "Downloading langchain_experimental-0.0.53-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.7/173.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain_experimental\n",
      "Successfully installed langchain_experimental-0.0.53\n",
      "Requirement already satisfied: langchain[docarray] in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (0.1.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (0.6.4)\n",
      "Collecting docarray<0.33.0,>=0.32.0 (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Downloading docarray-0.32.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (0.0.27)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (0.1.30)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (0.1.22)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain[docarray]) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (0.9.0)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (3.9.15)\n",
      "Collecting rich>=13.1.0 (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting types-requests>=2.28.11.6 (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Downloading types_requests-2.31.0.20240218-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting protobuf>=3.19.0 (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain[docarray]) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.29->langchain[docarray]) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.29->langchain[docarray]) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain[docarray]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain[docarray]) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain[docarray]) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (2024.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain[docarray]) (1.3.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (2.17.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (1.0.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading docarray-0.32.1-py3-none-any.whl (215 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.31.0.20240218-py3-none-any.whl (14 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for hnswlib \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[66 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m creating var\n",
      "  \u001b[31m   \u001b[0m creating var/folders\n",
      "  \u001b[31m   \u001b[0m creating var/folders/gc\n",
      "  \u001b[31m   \u001b[0m creating var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn\n",
      "  \u001b[31m   \u001b[0m creating var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T\n",
      "  \u001b[31m   \u001b[0m clang -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/mzihayat/VSC/Prompt_Engineering/.venv/include -I/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/include/python3.11 -c /var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/tmpq3v_3jpt.cpp -o var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/tmpq3v_3jpt.o -std=c++14\n",
      "  \u001b[31m   \u001b[0m xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\n",
      "  \u001b[31m   \u001b[0m clang -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -I/Users/mzihayat/VSC/Prompt_Engineering/.venv/include -I/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/include/python3.11 -c /var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/tmp7ls_swsk.cpp -o var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/tmp7ls_swsk.o -std=c++11\n",
      "  \u001b[31m   \u001b[0m xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 410, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._build_with_temp_dir(\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 395, in _build_with_temp_dir\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 311, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 131, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/__init__.py\", line 103, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 963, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/wheel/bdist_wheel.py\", line 368, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 963, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 963, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 89, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/gc/2wrphvw94cl1stzcrbvpd7kw0000gn/T/pip-build-env-g6tfg2p5/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 101, in build_extensions\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 70, in cpp_flag\n",
      "  \u001b[31m   \u001b[0m RuntimeError: Unsupported compiler -- at least C++11 support is needed!\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for hnswlib\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build hnswlib\n",
      "\u001b[31mERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install tiktoken\n",
    "!pip install faiss-cpu\n",
    "!pip install langchain_experimental\n",
    "!pip install \"langchain[docarray]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loading for RAG System Setup**\n",
    "\n",
    "- **Environment Preparation:**\n",
    "  - Ensure your environment is fully set up and authenticated for OpenAI API access.\n",
    "\n",
    "- **Objective:**\n",
    "  - To develop a conversational AI capable of utilizing information from a specific text file.\n",
    "  - The text file, named `openai.txt`, contains detailed data about OpenAI Innovation.\n",
    "\n",
    "- **Accessing Data:**\n",
    "  - The `openai.txt` file is available for download for those who wish to follow the tutorial with the same dataset.\n",
    "\n",
    "- **RAG System Setup Steps:**\n",
    "  - The specifics of setting up the RAG system were to be detailed following this introduction.\n",
    "\n",
    "- **Goal:**\n",
    "  - Integrate the `openai.txt` data into the RAG system, enabling the conversational AI to provide informed responses based on OpenAI Innovation's information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file_path = 'data/openai.txt'\n",
    "loader = TextLoader(file_path=txt_file_path, encoding=\"utf-8\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Preprocessing Data for RAG System**\n",
    "\n",
    "- **Objective:**\n",
    "  - Break down the dataset into manageable chunks for efficient processing.\n",
    "\n",
    "- **Tool Used:**\n",
    "  - **CharacterTextSplitter:** Utilized for dividing the text file into smaller sections.\n",
    "\n",
    "- **Purpose of Preprocessing:**\n",
    "  - Ensures comprehensive processing by not overlooking any details.\n",
    "  - Facilitates the handling of data, making it more accessible for the RAG system.\n",
    "\n",
    "- **Benefits:**\n",
    "  - Enhances the efficiency of the RAG system in retrieving and utilizing information.\n",
    "  - Improves the conversational AI's ability to provide detailed and informed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "data = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Vector Store**\n",
    "  - The Vector Store is central to the RAG system, enabling advanced searchability and information retrieval.\n",
    "  - Text data is converted into **vectors**, or mathematical representations, allowing for precise and quick search operations.\n",
    "  - Acts like a highly organized digital library, where each piece of information is indexed based on its content for easy retrieval.\n",
    "\n",
    "- **Impact on Chatbot Performance:**\n",
    "  - Significantly improves the chatbot's ability to understand and respond to user queries with accurate information.\n",
    "  - Essential for the system's ability to handle complex inquiries and provide fact-based, detailed answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating the Vector Store with OpenAI Embeddings**\n",
    "\n",
    "- **Converting Text to Vectors:**\n",
    "  - **OpenAIEmbeddings:** A tool that transforms text data into vectors, akin to translating English into French, translating human-readable text into machine-understandable numerical language.\n",
    "  \n",
    "- **Mechanism:**\n",
    "  - Text is processed to capture its essence, including meaning and context, in numerical form.\n",
    "  \n",
    "- **Purpose and Benefit:**\n",
    "  - These vectors are designed for quick processing by computational models, enhancing efficiency and accuracy in information retrieval.\n",
    "  \n",
    "- **Application in RAG System:**\n",
    "  - By converting text to vectors, the RAG system's Vector Store becomes a powerful tool for searching and retrieving information based on the semantic content of user queries.\n",
    "\n",
    "- **Impact on Conversational AI:**\n",
    "  - Enables the chatbot to understand and respond to queries more effectively, leveraging deep understanding of the text's meaning and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Storing Vectors with FAISS**\n",
    "\n",
    "- **Introduction to FAISS:**\n",
    "  - Developed by Facebook AI Research lab, FAISS stands for Facebook AI Similarity Search.\n",
    "  - A library designed for storing and searching through vectors efficiently.\n",
    "\n",
    "- **Functionality of FAISS:**\n",
    "  - Enables almost instantaneous search capabilities within large datasets.\n",
    "  - Comparable to a librarian who can instantly direct you to a book in a library, FAISS quickly locates the most relevant data points based on a query.\n",
    "\n",
    "- **Benefits for RAG System:**\n",
    "  - Dramatically reduces the time needed to find relevant information within the Vector Store.\n",
    "  - Enhances the AI's ability to deliver precise responses by quickly identifying the best match for a user's query.\n",
    "\n",
    "- **Application in Conversational AI:**\n",
    "  - By leveraging FAISS, the chatbot can access and retrieve specific information from the Vector Store with minimal delay.\n",
    "  - Ensures that the chatbot’s responses are not only relevant but also timely, improving user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(data, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vector Store: A Powerful Search Engine for Chatbots**\n",
    "  - Utilizes OpenAIEmbeddings for converting text into vectors.\n",
    "  - Employs FAISS for efficient storage and rapid retrieval of these vectors.\n",
    "\n",
    "- **Capabilities of the Vector Store:**\n",
    "  - Acts as a sophisticated search engine, navigating through vast text databases.\n",
    "  - Identifies the most relevant information quickly in response to user queries.\n",
    "\n",
    "- **Enhanced Retrieval Mechanism:**\n",
    "  - Equips the RAG system with advanced capabilities to provide precise, context-aware responses.\n",
    "  - Analogous to an expert using a well-organized system to retrieve and answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bringing the RAG Chatbot to Life with a Conversation Chain**\n",
    "  - Shifts the RAG system from a static information repository to a dynamic conversational partner.\n",
    "\n",
    "- **Utilizing the LangChain Library:**\n",
    "  - Employs LangChain, a versatile toolkit, to build conversational AI applications.\n",
    "  - Leverages the power of language models to facilitate engaging dialogues.\n",
    "\n",
    "- **Impact of the Conversation Chain:**\n",
    "  - Enables the chatbot to interact in a more natural, human-like manner.\n",
    "  - Enhances the chatbot's ability to understand and respond to complex queries effectively.\n",
    "\n",
    "- **Outcome for Users:**\n",
    "  - Users experience a seamless and engaging interaction with the chatbot.\n",
    "  - The chatbot provides timely, relevant, and contextually appropriate responses, improving user satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understanding the Conversation Chain**\n",
    "  - Acts as the chatbot's \"stream of thought.\"\n",
    "  - A sequence of steps from query understanding to delivering a relevant response.\n",
    "\n",
    "- **Function and Importance:**\n",
    "  - Central to making the chatbot appear intelligent and responsive.\n",
    "  - Enables the chatbot to process and engage with user queries in a coherent manner.\n",
    "\n",
    "- **How It Works:**\n",
    "  - Begins with the chatbot interpreting the user's question.\n",
    "  - Continues with searching for and selecting appropriate information.\n",
    "  - Ends with formulating and presenting an answer to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building the Conversation Chain with ChatOpenAI**\n",
    "\n",
    "- **Initializing the Conversational Model:**\n",
    "  - Utilize ChatOpenAI, specifically selecting GPT-4 for its advanced text generation capabilities.\n",
    "  - GPT-4 is known for its superior understanding and generation of human-like text, making it ideal for conversational AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Integrating Memory with ConversationBufferMemory**\n",
    "  - Just like humans,  the conversation should remember previous exchanges.\n",
    "  - Essential for maintaining context throughout the interaction.\n",
    "\n",
    "- **Functionality of ConversationBufferMemory:**\n",
    "  - Stores the history of the chatbot's interactions with users.\n",
    "  - Enables the chatbot to reference past conversation points, enhancing response relevance.\n",
    "\n",
    "- **Benefits to Conversational AI:**\n",
    "  - Ensures responses are informed by earlier dialogue, improving coherence and continuity.\n",
    "  - Enhances user experience by providing context-aware interactions, making the chatbot appear more intelligent and attentive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linking Components into a ConversationalRetrievalChain**\n",
    "  - Combines the conversational model and ConversationBufferMemory to enhance dialogue capabilities.\n",
    "\n",
    "- **Role of ConversationalRetrievalChain:**\n",
    "  - Orchestrates the response generation process by leveraging the Vector Store for information retrieval.\n",
    "  - Ensures the formulation of coherent and contextually relevant replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangChain: Enhancing Chatbot Capabilities**\n",
    "  - LangChain provides the structure to make the chatbot context-aware, enhancing its understanding of the conversation flow.\n",
    "  - Facilitates connection to external context sources, such as the Vector Store, enriching the chatbot's responses with external knowledge.\n",
    "  - Allows the chatbot to remember and reference past interactions, ensuring continuity and relevance in conversations.\n",
    "\n",
    "- With LangChain, the chatbot transcends being a mere information source to become a helpful conversational partner.\n",
    "\n",
    "- Enables the chatbot to access and retrieve detailed information from sources like `openai.txt`, effectively using OpenAI Innovation's knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Interacting with the RAG System: Example Queries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OpenAI Innovation is a company that specializes in Generative AI and Large Language Models. They focus on integrating these technologies into business strategies, offering tailored solutions that enhance innovation and operational efficiency. Their proficiency extends across multiple industry verticals, helping businesses to harness the power of AI-driven digital transformation. They automate workflows, perform content analysis, implement custom models, and are committed to ethical compliance. Their services are adaptable to diverse industrial requirements and challenges, making them a trusted partner for businesses worldwide."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is OpenAI Innovation?\"\n",
    "result = conversation_chain({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "answer\n",
    "Markdown(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Query 2: Getting Contact Details**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OpenAI Innovation can be contacted through the following means:\n",
       "\n",
       "Address: Innovation Park, 1234 Discovery Drive, Techville, Silicon State, 98765\n",
       "\n",
       "Email Address: contact@openai.com\n",
       "\n",
       "Phone Number: +1-555-0123-456\n",
       "\n",
       "Fax: +1-555-0123-457\n",
       "\n",
       "You can also reach them on WhatsApp at +216-55-770-606 or send an email at info@OpenAIi.ai.\n",
       "\n",
       "Their headquarters is located at Route Mahdia km 0.5, Pavillon d’Or Building, 3000 Sfax, Tunisia."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the contact information?\"\n",
    "result = conversation_chain({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Query 3: Identifying Key Activities**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- Specializing in Generative AI and Large Language Models to provide tailored solutions that enhance business innovation and operational efficiency.\n",
       "- Applying AI technologies across multiple industry verticals to automate workflows, perform content analysis, and implement custom models.\n",
       "- Offering cross-domain consultation, business automation, and a client-centric approach to bridge the gap between technology and business."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the main activities of OpenAI Innovation. Write is as three bullet points.\"\n",
    "result = conversation_chain({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "Markdown(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
