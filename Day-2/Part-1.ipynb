{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outline**\n",
    "\n",
    "- Prompting Techniques: Zero-shot, Few-shot, Chain-of-Thought\n",
    "- Hands-On Exercises: Crafting Complex Prompts\n",
    "- Introduction to Prompt Optimization Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prompting Techniques**\n",
    "\n",
    "- Prompt Engineering optimizes prompt design for enhanced LLM task performance.\n",
    "- Transition from basic to advanced techniques for complex task achievement.\n",
    "- Focus on improving LLM reliability and performance through advanced prompting.\n",
    "\n",
    "- There are different techniques of prompting\n",
    "- the most popular and common ones are:\n",
    "  - Zero-shot \n",
    "  - Few-shot \n",
    "  - Chain-of-Though\n",
    "  - Self-Consistency\n",
    "  - Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Zero-Shot Prompting**\n",
    "\n",
    "- Large LLMs like GPT-3 excel in \"zero-shot\" tasks due to extensive training and data.\n",
    "- Capable of understanding and executing instructions without prior examples.\n",
    "- Illustration provided with a zero-shot example from the previous section.\n",
    "\n",
    "```\n",
    "Classify the text into neutral, negative or positive. \n",
    "\n",
    "Text: I think the vacation is okay.\n",
    "Sentiment:\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "Neutral\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zero-shot capability allows LLMs to understand tasks like \"sentiment\" analysis without prior examples.\n",
    "- Instruction tuning can enhance zero-shot learning, as demonstrated by Wei et al. (2022).\n",
    "- This involves finetuning models with datasets described through instructions.\n",
    "- RLHF (reinforcement learning from human feedback) scales instruction tuning by aligning models with human preferences, exemplified by ChatGPT.\n",
    "  - Discussion on instruction tuning and RLHF methods to follow in upcoming sections.\n",
    "- For tasks where zero-shot is insufficient, \n",
    "  - Few-shot prompting with examples improves performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Few-Shot Prompting**\n",
    "\n",
    "- Large language models excel in zero-shot tasks but may struggle with complex challenges.\n",
    "- Few-shot prompting enhances performance by providing **in-context** learning through examples.\n",
    "- Demonstrations in prompts serve as conditioning for better model responses.\n",
    "- Few-shot capabilities emerged as models scaled in size, per Kaplan et al., 2020.\n",
    "- Example from Brown et al., 2020, illustrates few-shot prompting to use a new word correctly in a sentence.\n",
    "```\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    " \n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "\n",
    "```\n",
    "\n",
    "- The model can learn tasks from a single example, known as 1-shot learning.\n",
    "- For complex tasks, effectiveness may improve with more examples (3-shot, 5-shot, 10-shot, etc.).\n",
    "- Experimentation with the number of demonstrations can optimize task performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demonstrations' label space and input text distribution significantly impact model performance, as found by Min et al. (2022).\n",
    "- Correctness of labels for individual inputs is less critical than the overall structure provided by demonstrations.\n",
    "- The format of demonstrations, including the use of labels, enhances performance—even random labels are beneficial.\n",
    "- Opting for a true distribution of labels over a uniform distribution further improves task execution.\n",
    "\n",
    "- Experimenting with random label assignments to inputs.\n",
    "- Utilizes labels \"Negative\" and \"Positive\" without regard to their actual relevance.\n",
    "- This approach tests the impact of label format on model understanding and response quality.\n",
    "\n",
    "- **Example**\n",
    "```\n",
    "This is awesome! // Negative\n",
    "This is bad! // Positive\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! //\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correct answers achieved despite randomized label assignment.\n",
    "- Maintaining a consistent format contributes to successful outcomes.\n",
    "- Newer GPT models exhibit increased robustness to variations in formatting.\n",
    "- Further experimentation reveals improved model adaptability to random formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Positive This is awesome! \n",
    "This is bad! Negative\n",
    "Wow that movie was rad!\n",
    "Positive\n",
    "What a horrible show! --\n",
    "```\n",
    "\n",
    "- The model correctly predicts labels despite inconsistent formatting.\n",
    "- Indicates newer GPT models' ability to interpret and adapt to varied prompt structures.\n",
    "- A more detailed analysis is required to assess performance across diverse and complex tasks.\n",
    "- Investigating different prompt variations will help understand model robustness and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Limitations of Few-shot Prompting**\n",
    "- Few-shot prompting is effective for various tasks but has limitations with complex reasoning.\n",
    "- Demonstrating these limitations through a previously discussed example task.\n",
    "- The example will illustrate the challenges few-shot prompting faces with intricate reasoning scenarios.\n",
    "\n",
    "```\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "\n",
    "A: \n",
    "```\n",
    "Output:\n",
    "```\n",
    "Yes, the odd numbers in this group add up to 107, which is an even number.\n",
    "```\n",
    "\n",
    "- Incorrect responses underscore the limitations of current language models.\n",
    "- Indicates a pressing need for advanced prompt engineering techniques.\n",
    "- Enhancing prompt design can address shortcomings in complex task handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Limitations of Few-shot Prompting**\n",
    "\n",
    "- Another example:\n",
    "```\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "The answer is True.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Limitations of Few-shot Prompting**\n",
    "\n",
    "- Few-shot prompting shows limitations in handling complex reasoning problems.\n",
    "- The task's complexity requires breaking down the problem into manageable steps.\n",
    "- Chain-of-Thought (CoT) prompting emerges as a solution for intricate tasks involving arithmetic, commonsense, and symbolic reasoning.\n",
    "- Demonstrating the problem-solving process step-by-step helps the model generate more accurate and reliable responses.\n",
    "\n",
    "- Examples in prompts aid in solving certain tasks effectively.\n",
    "- Limitations of zero-shot and few-shot prompting suggest a need for further model learning or adaptation.\n",
    "- Fine-tuning models or exploring advanced prompting techniques becomes necessary for improvement.\n",
    "- Introduction to Chain-of-Thought (CoT) prompting as a next step, highlighting its growing popularity in tackling complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chain-of-Thought (CoT) Prompting**\n",
    "\n",
    "- Chain-of-Thought (CoT) prompting enhances complex reasoning by outlining intermediate steps.\n",
    "- Combining CoT with few-shot prompting improves outcomes on tasks needing detailed reasoning.\n",
    "- This approach aids models in navigating through complex problems before generating responses.\n",
    "\n",
    "\n",
    "<img src=\"./images/cot.webp\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "Image Source: Wei et al. (2022)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Providing reasoning steps significantly improves model accuracy.\n",
    "- Demonstrates the effectiveness of CoT prompting with minimal examples.\n",
    "- A single example can suffice to guide the model to correct conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\n",
    "```\n",
    "\n",
    "- The emergence of advanced reasoning capabilities is attributed to the scale of language models.\n",
    "- Authors note that sufficiently large models develop the ability to follow complex reasoning paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Zero-shot COT Prompting**\n",
    "\n",
    "- Zero-shot Chain-of-Thought (CoT) introduces \"Let's think step by step\" to prompts.\n",
    "- Concept by Kojima et al. 2022 aims to enable reasoning without prior examples.\n",
    "- Testing this approach with a simple problem evaluates the model's reasoning performance.\n",
    "\n",
    "<img src=\"./images/zero-cot.webp\" width=\"800\" align=\"center\"/>\n",
    "Image Source: Kojima et al. (2022)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "```\n",
    " \n",
    "- The answer is incorrect! Now Let's try with the special prompt.\n",
    "\n",
    "``` \n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "\n",
    "```\n",
    "\n",
    "- The simplicity and effectiveness of the zero-shot CoT prompt are notable.\n",
    "- Valuable for situations with limited examples available for prompting.\n",
    "- Demonstrates the potential for efficient reasoning in minimal-setup scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Self-Consistency**\n",
    "  - Proposed by Wang et al. (2022) to enhance Chain-of-Thought (CoT) prompting.\n",
    "  - Moves beyond simplistic greedy decoding by incorporating multiple, diverse reasoning paths.\n",
    "  - Technique involves sampling various reasoning paths via few-shot CoT.\n",
    "  - The most consistent answer among these generations is selected.\n",
    "  - Significantly improves performance on arithmetic and commonsense reasoning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "When I was 6 my sister was half my age. Now\n",
    "I’m 70 how old is my sister?\n",
    "```\n",
    "\n",
    "> The output is wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A:\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
