{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outline**\n",
    "\n",
    "- Generating Data and Synthetic Datasets\n",
    "- Case Study: Graduate Job Classification\n",
    "- Workshop: Tackling Generated Datasets Diversity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv openai-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source openai-env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (1.13.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from openai) (2.6.1)\n",
      "Requirement already satisfied: sniffio in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/mzihayat/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI()\n",
    "# defaults to getting the key using os.environ.get(\"OPENAI_API_KEY\")\n",
    "# if you saved the key under a different environment variable name, you can do something like:\n",
    "client = OpenAI(\n",
    "  api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"In the realm of coding, there's a method quite grand,\\nA concept mysterious, yet elegantly planned.\\nRecursion it's called, a looping of delight,\\nA function that calls itself, shining bright.\\n\\nLike a mirror reflecting endlessly,\\nRecursion spirals through functions, fluently.\\nIn a dance of repetition, it finds its grace,\\nSolving problems with an elegant embrace.\\n\\nThe base case, a safety net in its design,\\nControls the looping, so sublime.\\nAs calls unravel, deeply nested in the code,\\nRecursion weaves a tale, elegantly told.\\n\\nFrom fractals to trees, it finds its place,\\nA concept so profound, yet full of grace.\\nSo embrace the mystery, let recursion guide your way,\\nIn the enchanted realm of coding, where its magic holds sway.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Compose a poem that explains the concept of recursion in programming.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function Calling with LLMs**\n",
    "  - Enables LLMs like GPT-4 and GPT-3.5 to reliably connect with external tools and APIs.\n",
    "  - Detects the need for a function call within a chat and outputs JSON with arguments to execute the function.\n",
    "\n",
    "- **Tool Integration:**\n",
    "  - Functions act as tools within AI applications, allowing for multiple tools to be defined and called in a single request.\n",
    "\n",
    "- **Importance for AI Applications:**\n",
    "  - Essential for developing LLM-powered chatbots or agents that require context retrieval or need to interact with external tools.\n",
    "  - Transforms natural language instructions into actionable API calls, enhancing the utility and interactivity of chatbots.\n",
    "\n",
    "- **Enhancing Chatbot Capabilities:**\n",
    "  - Facilitates seamless integration of LLMs with a wide range of external services and data sources.\n",
    "  - Enables chatbots to perform complex tasks, such as data retrieval, content creation, and more, by calling specific functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Applications Enabled by Functional Calling**\n",
    "\n",
    "- **Conversational Agents with External Tool Usage:**\n",
    "  - Allows conversational agents to efficiently utilize external tools to answer user questions.\n",
    "  - Example: Querying weather information translates to function calls like `get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')`.\n",
    "\n",
    "- **Data Extraction and Tagging Solutions:**\n",
    "  - Empowers LLM-powered solutions to extract and tag data from various sources.\n",
    "  - Example: Extracting people names from a Wikipedia article.\n",
    "\n",
    "- **Natural Language to API Conversion:**\n",
    "  - Facilitates the creation of applications that translate natural language into API calls or database queries.\n",
    "  - Enhances the usability and accessibility of data and services.\n",
    "\n",
    "- **Conversational Knowledge Retrieval Engines:**\n",
    "  - Enables conversational engines to interact with knowledge bases, facilitating knowledge retrieval through natural language queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function Calling with GPT-4**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Integration of LLM with External Tool for Weather Query**\n",
    "  - **Challenge:**\n",
    "    - LLM alone cannot respond to dynamic queries like checking the weather due to dataset limitations.\n",
    "  - **Solution:**\n",
    "    - Utilize function calling capabilities of the LLM to invoke an external tool for weather information retrieval.\n",
    "\n",
    "  - **Implementation Example:**\n",
    "    - User query: \"What is the weather like in a given location?\"\n",
    "    - LLM processes the query and recognizes the need for external information.\n",
    "    - Function calling mechanism selects appropriate function and arguments (e.g., `get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')`).\n",
    "    - OpenAI APIs facilitate the interaction between the LLM and the weather API.\n",
    "    - Final response generated based on the retrieved weather data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "What is the weather like in London?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To handle this request using function calling,\n",
    "  - Define a weather function or set of functions that you will be passing as part of the OpenAI API request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function Details:\n",
    "  - Name: get_current_weather\n",
    "  - Description: Retrieves the current weather in a specified location.\n",
    "  - Parameters:\n",
    "    - location: Specifies the city and state (e.g., San Francisco, CA).\n",
    "    - unit: Specifies the temperature unit (celsius or fahrenheit).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a completion function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(client, messages, model=\"gpt-3.5-turbo-1106\", temperature=0, max_tokens=300, tools=None, tool_choice=None):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        tools=tools,\n",
    "        tool_choice=tool_choice\n",
    "    )\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose the user question:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the weather like in London?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_q17T7UG98lVTh9XMuZcb0yrv', function=Function(arguments='{\"location\":\"London\",\"unit\":\"celsius\"}', name='get_current_weather'), type='function')])\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(client, messages, tools=tools)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capture the arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args = json.loads(response.tool_calls[0].function.arguments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a dummy function to get the current weather\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"50\",\n",
    "        \"unit\": unit,\n",
    "    }\n",
    "\n",
    "    return json.dumps(weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": \"London\", \"temperature\": \"50\", \"unit\": \"celsius\"}'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_weather(**args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **An LLM-powered conversational agent**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello! How are you?\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_completion() missing 1 required positional argument: 'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_completion(messages, tools\u001b[39m=\u001b[39;49mtools)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_completion() missing 1 required positional argument: 'messages'"
     ]
    }
   ],
   "source": [
    "get_completion(messages, tools=tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specify desired behavior for function calling to tailor the response generation process according to specific requirements.\n",
    "- By default, the model autonomously determines whether to call a function and which function to call.\n",
    "-  Utilize the tool_choice parameter to control the behavior of the system.\n",
    "- Default setting: tool_choice: \"auto\".\n",
    "\n",
    "- In \"auto\" mode, the model automatically decides whether and which function to call based on the context of the user query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=\"Hello! I'm here and ready to assist you. How can I help you today?\", role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(client, messages, tools=tools, tool_choice=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=\"Hello! I'm here and ready to assist you. How can I help you today?\", role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(client, messages, tools=tools, tool_choice=\"none\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forces the model to not use any of the functions provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='I will check the current weather in London for you.', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in London?\",\n",
    "    }\n",
    "]\n",
    "get_completion(client, messages, tools=tools, tool_choice=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force the model to choose a function if that's the behavior you want in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_dh0nudIqqUzA3YLm51IXh2EJ', function=Function(arguments='{\"location\":\"London\",\"unit\":\"celsius\"}', name='get_current_weather'), type='function')])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in London?\",\n",
    "    }\n",
    "]\n",
    "get_completion(client, messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Develop an agent that passes back the result obtained after calling your APIs with the inputs generated from function calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's the weather like in Boston!\"})\n",
    "assistant_message = get_completion(client, messages, tools=tools, tool_choice=\"auto\")\n",
    "assistant_message = json.loads(assistant_message.model_dump_json())\n",
    "assistant_message[\"content\"] = str(assistant_message[\"tool_calls\"][0][\"function\"])\n",
    "\n",
    "#a temporary patch but this should be handled differently\n",
    "# remove \"function_call\" from assistant message\n",
    "del assistant_message[\"function_call\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(assistant_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weather information to pass back to the model\n",
    "weather = get_current_weather(messages[1][\"tool_calls\"][0][\"function\"][\"arguments\"])\n",
    "\n",
    "messages.append({\"role\": \"tool\",\n",
    "                 \"tool_call_id\": assistant_message[\"tool_calls\"][0][\"id\"],\n",
    "                 \"name\": assistant_message[\"tool_calls\"][0][\"function\"][\"name\"],\n",
    "                 \"content\": weather})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = get_completion(client, messages, tools=tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='The current weather in Boston, MA is 50°F.', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating Data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LLMs possess powerful text generation capabilities for creating coherent text.\n",
    "- Implementing effective prompt strategies enhances the quality, consistency, and factual accuracy of the model's outputs.\n",
    "- LLMs are invaluable for generating datasets, facilitating a wide range of experiments and evaluations.\n",
    "- They can be utilized for quick sample generation in applications, such as training a sentiment classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Produce 10 exemplars for sentiment analysis. Examples are categorized as either positive or negative. Produce 2 negative examples and 8 positive examples. Use this format for the examples:\n",
    "Q: <sentence>\n",
    "A: <sentiment>\n",
    "```\n",
    "\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Q: I just got the best news ever!\n",
    "A: Positive\n",
    "\n",
    "Q: We just got a raise at work!\n",
    "A: Positive\n",
    "\n",
    "Q: I'm so proud of what I accomplished today.\n",
    "A: Positive\n",
    "\n",
    "Q: I'm having the best day ever!\n",
    "A: Positive\n",
    "\n",
    "Q: I'm really looking forward to the weekend.\n",
    "A: Positive\n",
    "\n",
    "Q: I just got the best present ever!\n",
    "A: Positive\n",
    "\n",
    "Q: I'm so happy right now.\n",
    "A: Positive\n",
    "\n",
    "Q: I'm so blessed to have such an amazing family.\n",
    "A: Positive\n",
    "\n",
    "Q: The weather outside is so gloomy.\n",
    "A: Negative\n",
    "\n",
    "Q: I just got some terrible news.\n",
    "A: Negative\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Learning projects often face challenges due to insufficient labeled data, leading to prolonged data collection and labeling phases.\n",
    "- The emergence of LLMs has transformed this paradigm, enabling the testing and development of ideas or AI-powered features with minimal delay.\n",
    "- LLMs leverage their generalization capabilities to provide immediate insights and preliminary results.\n",
    "- Successful initial testing with LLMs can justify and lead into the traditional, more time-consuming development process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/synthetic_rag_1.webp\" width=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retrieval Augmented Generation (RAG) is a method that combines\n",
    "  -  information retrieval with LLM text generation for knowledge-intensive tasks.\n",
    "- The Retrieval model is key,\n",
    "  - Selecting relevant documents for the LLM to process, with its performance directly impacting the quality of the output.\n",
    "- RAG's effectiveness can vary across languages and specific domains, \n",
    "  - Sometimes struggling with tasks like creating a chatbot for Czech legal advice or a tax assistant for the Indian market.\n",
    "- A solution to enhance RAG's performance \n",
    "  - Using LLMs to synthesize training data for new models, a method that can improve accuracy in underrepresented languages or specialized areas.\n",
    "- This approach, while computationally demanding, aims at distilling LLMs into more efficient models, potentially lowering inference costs and boosting overall system performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Domain-Specific Dataset Generation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilizing LLMs for retrieval tasks necessitates providing a concise description and a handful of manually labeled examples.\n",
    "- Different retrieval tasks have distinct search intents, impacting the definition of \"relevance\" for any given (Query, Document) pair.\n",
    "- The perceived relevance of a document can vary significantly based on the specific search intent behind the retrieval task.\n",
    "- For example, an argument retrieval task may look for documents containing supporting arguments, whereas other tasks may seek counter-arguments, as illustrated by the ArguAna dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Task: Identify a counter-argument for the given argument.\n",
    "\n",
    "Argument #1: {insert passage X1 here}\n",
    "\n",
    "A concise counter-argument query related to the argument #1: {insert manually prepared query Y1 here}\n",
    "\n",
    "Argument #2: {insert passage X2 here}\n",
    "A concise counter-argument query related to the argument #2: {insert manually prepared query Y2 here}\n",
    "\n",
    "<- paste your examples here ->\n",
    "\n",
    "Argument N: Even if a fine is made proportional to income, you will not get the equality of impact you desire. This is because the impact is not proportional simply to income, but must take into account a number of other factors. For example, someone supporting a family will face a greater impact than someone who is not, because they have a smaller disposable income. Further, a fine based on income ignores overall wealth (i.e. how much money someone actually has: someone might have a lot of assets but not have a high income). The proposition does not cater for these inequalities, which may well have a much greater skewing effect, and therefore the argument is being applied inconsistently.\n",
    "\n",
    "A concise counter-argument query related to the argument #N:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "punishment house would make fines relative income\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/synthetic_rag_2.webp\" width=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prioritize responsible manual annotation, preparing around **20 examples** and randomly **selecting 2-8 for prompts** to enhance data diversity.\n",
    "- Ensure examples are representative, correctly formatted, and detail specifics like query length and tone.\n",
    "- Precision in examples and instructions improves synthetic data quality for Retriever training.\n",
    "- Low-quality few-shot examples can detrimentally affect the trained model's quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilizing an affordable model like ChatGPT is often adequate for non-English languages and unusual domains.\n",
    "- A typical prompt with instructions and 4-5 examples might use about 700 tokens, plus 25 tokens for generation, under Retriever's 128-token constraint per passage.\n",
    "- The cost of generating a synthetic dataset for 50,000 documents, considering GPT-3.5 Turbo API pricing, is approximately $55.\n",
    "  - Calculation: 50,000 * (700 * 0.001 * $0.0015 + 25 * 0.001 * $0.002)\n",
    "- Generating multiple (2-4) query examples per document is feasible and can enhance local model fine-tuning.\n",
    "- Further training, despite additional costs, often yields significant benefits, particularly for specialized domains such as Czech law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb Cell 57\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Generate data for each genre\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m num_samples_per_genre \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m  \u001b[39m# You can choose how many samples you want to generate\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m sci_fi_data \u001b[39m=\u001b[39m generate_synthetic_data(client, \u001b[39m'\u001b[39;49m\u001b[39mScience Fiction\u001b[39;49m\u001b[39m'\u001b[39;49m, num_samples_per_genre)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m romance_data \u001b[39m=\u001b[39m generate_synthetic_data(client, \u001b[39m'\u001b[39m\u001b[39mRomance\u001b[39m\u001b[39m'\u001b[39m, num_samples_per_genre)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Combine the data\u001b[39;00m\n",
      "\u001b[1;32m/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb Cell 57\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m synthetic_data \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_samples):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo-1106\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompts[genre],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     synthetic_data\u001b[39m.\u001b[39mappend((response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m], genre))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mzihayat/VSC/Prompt_Engineering/Day-3/Part-1.ipynb#Y111sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m synthetic_data\n",
      "File \u001b[0;32m~/VSC/Prompt_Engineering/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    275\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Missing required arguments; Expected either ('messages' and 'model') or ('messages', 'model' and 'stream') arguments to be given"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Function to generate synthetic data\n",
    "def generate_synthetic_data(client,  genre, num_samples):\n",
    "    prompts = {\n",
    "        'Science Fiction': 'Write a science fiction story about a robot uprising.',\n",
    "        'Romance': 'Write a romance story set in a small coastal town.'\n",
    "    }\n",
    "\n",
    "    synthetic_data = []\n",
    "    for _ in range(num_samples):\n",
    "        response = client.chat.completions.create(\n",
    "            engine=\"gpt-3.5-turbo-1106\",\n",
    "            prompt=prompts[genre],\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        synthetic_data.append((response['choices'][0]['text'], genre))\n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "# Generate data for each genre\n",
    "num_samples_per_genre = 50  # You can choose how many samples you want to generate\n",
    "sci_fi_data = generate_synthetic_data(client, 'Science Fiction', num_samples_per_genre)\n",
    "romance_data = generate_synthetic_data(client, 'Romance', num_samples_per_genre)\n",
    "\n",
    "# Combine the data\n",
    "all_data = sci_fi_data + romance_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
